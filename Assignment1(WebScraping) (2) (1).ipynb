{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54345786",
   "metadata": {},
   "source": [
    " Write a python program to display all the header tags from wikipedia.org.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc8225a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 : Main Page\n",
      "h1 : Welcome to Wikipedia\n",
      "h2 : From today's featured article\n",
      "h2 : Did you know ...\n",
      "h2 : In the news\n",
      "h2 : On this day\n",
      "h2 : Today's featured picture\n",
      "h2 : Other areas of Wikipedia\n",
      "h2 : Wikipedia's sister projects\n",
      "h2 : Wikipedia languages\n",
      "h2 : Navigation menu\n",
      "h3 : Personal tools\n",
      "h3 : Namespaces\n",
      "h3 : Views\n",
      "h3 : Navigation\n",
      "h3 : Contribute\n",
      "h3 : Tools\n",
      "h3 : Print/export\n",
      "h3 : In other projects\n",
      "h3 : Languages\n"
     ]
    }
   ],
   "source": [
    "#BeautifulSoup Assignment-1 Question-1\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "# scraping a wikipedia article to list all tags\n",
    "url_link = 'https://en.wikipedia.org/wiki/Main_Page' \n",
    "request = requests.get(url_link)\n",
    " \n",
    "soup = BeautifulSoup(request.text, 'html5lib') #Parsing\n",
    " \n",
    "# creating a list of all common heading tags for matching \n",
    "\n",
    "head_tags = [\"h1\", \"h2\", \"h3\" ,\"h4\", \"h5\" , \"h6\"]\n",
    "\n",
    "for tags in soup.find_all(head_tags):\n",
    "    print(tags.name + ' : ' + tags.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d02698f",
   "metadata": {},
   "source": [
    " Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08035187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Year Of Realese</th>\n",
       "      <th>Imdb Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1972</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>1974</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>1957</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Citizen Kane</td>\n",
       "      <td>1941</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>M - Eine Stadt sucht einen Mörder</td>\n",
       "      <td>1931</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Lawrence of Arabia</td>\n",
       "      <td>1962</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>1959</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Vertigo</td>\n",
       "      <td>1958</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Movie Name  Year Of Realese  Imdb Rating\n",
       "0            The Shawshank Redemption             1994         9.2\n",
       "1                       The Godfather             1972         9.2\n",
       "2                     The Dark Knight             2008         9.0\n",
       "3               The Godfather Part II             1974         9.0\n",
       "4                        12 Angry Men             1957         9.0\n",
       "..                                ...              ...         ...\n",
       "95                       Citizen Kane             1941         8.3\n",
       "96  M - Eine Stadt sucht einen Mörder             1931         8.3\n",
       "97                 Lawrence of Arabia             1962         8.3\n",
       "98                 North by Northwest             1959         8.2\n",
       "99                            Vertigo             1958         8.2\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BeautifulSoup Assignment-1 Question-2\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# scraping Imdb Website for top 100 movies\n",
    "\n",
    "url_link = 'https://www.imdb.com/chart/top/'\n",
    "\n",
    "request = requests.get(url_link)\n",
    "\n",
    "#Creating Empty lists for storing data\n",
    "\n",
    "namelist1=[]\n",
    "namelist2=[]\n",
    "realeselist3=[]\n",
    "list4=[]\n",
    "ratinglist5=[]\n",
    "\n",
    "soup = BeautifulSoup(request.text, 'html5lib')\n",
    "\n",
    "for title in soup.find_all('td',class_=\"titleColumn\"):\n",
    "    namelist1.append(title.text)\n",
    "\n",
    "for ratings in soup.find_all('td',class_=\"imdbRating\"):\n",
    "    list4.append(ratings.text)\n",
    "\n",
    "\n",
    "for i in range(0,100):\n",
    "\n",
    "    fname=namelist1[i].split()                      #Processing Movie Name and removing extra data\n",
    "    newfname=fname[1:len(fname)-1]\n",
    "    final_name=(\" \".join(newfname))\n",
    "    namelist2.append(final_name)\n",
    "    \n",
    "    realese_year=re.sub(r\"[\\([{})\\]]\", \"\",fname[len(fname)-1])\n",
    "    realeselist3.append(realese_year)                            #Fetching Realese Year from split data and reving brackets\n",
    "    \n",
    "    rating = list4[i].strip()\n",
    "    ratinglist5.append(rating)                       #Dropping extra data associated with rating\n",
    "\n",
    "#Creating Data Frame as asked for displaying data in readable format\n",
    "\n",
    "df = pd.DataFrame({\"Movie Name \": namelist2,\"Year Of Realese \": realeselist3 , \"Imdb Rating\" : ratinglist5})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d7cfee",
   "metadata": {},
   "source": [
    "Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c8abff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Year Of Realese</th>\n",
       "      <th>Imdb Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ramayana: The Legend of Prince Rama</td>\n",
       "      <td>1993</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rocketry: The Nambi Effect</td>\n",
       "      <td>2022</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Golmaal</td>\n",
       "      <td>1979</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nayakan</td>\n",
       "      <td>1987</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>777 Charlie</td>\n",
       "      <td>2022</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Kaakkaa Muttai</td>\n",
       "      <td>2014</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Ustad Hotel</td>\n",
       "      <td>2012</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Theeran Adhigaaram Ondru</td>\n",
       "      <td>2017</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Angoor</td>\n",
       "      <td>1982</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Baahubali 2: The Conclusion</td>\n",
       "      <td>2017</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Movie Name  Year Of Realese  Imdb Rating\n",
       "0   Ramayana: The Legend of Prince Rama             1993         8.5\n",
       "1            Rocketry: The Nambi Effect             2022         8.4\n",
       "2                               Golmaal             1979         8.4\n",
       "3                               Nayakan             1987         8.4\n",
       "4                           777 Charlie             2022         8.4\n",
       "..                                  ...              ...         ...\n",
       "95                       Kaakkaa Muttai             2014         8.0\n",
       "96                          Ustad Hotel             2012         8.0\n",
       "97             Theeran Adhigaaram Ondru             2017         8.0\n",
       "98                               Angoor             1982         8.0\n",
       "99          Baahubali 2: The Conclusion             2017         8.0\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BeautifulSoup Assignment-1 Question-3\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# scraping Imdb Website for top 100 indian movies\n",
    "\n",
    "url_link = 'https://www.imdb.com/india/top-rated-indian-movies/'\n",
    "\n",
    "request = requests.get(url_link)\n",
    "\n",
    "#Creating Empty lists for storing data\n",
    "\n",
    "namelist1=[]\n",
    "namelist2=[]\n",
    "realeselist3=[]\n",
    "list4=[]\n",
    "ratinglist5=[]\n",
    "\n",
    "soup = BeautifulSoup(request.text, 'html5lib')\n",
    "\n",
    "for title in soup.find_all('td',class_=\"titleColumn\"):\n",
    "    namelist1.append(title.text)\n",
    "\n",
    "for ratings in soup.find_all('td',class_=\"imdbRating\"):\n",
    "    list4.append(ratings.text)\n",
    "\n",
    "\n",
    "for i in range(0,100):\n",
    "\n",
    "    fname=namelist1[i].split()                      #Processing Movie Name and removing extra data\n",
    "    newfname=fname[1:len(fname)-1]\n",
    "    final_name=(\" \".join(newfname))\n",
    "    namelist2.append(final_name)\n",
    "    \n",
    "    realese_year=re.sub(r\"[\\([{})\\]]\", \"\",fname[len(fname)-1])\n",
    "    realeselist3.append(realese_year)                            #Fetching Realese Year from split data and reving brackets\n",
    "    \n",
    "    rating = list4[i].strip()\n",
    "    ratinglist5.append(rating)                       #Dropping extra data associated with rating\n",
    "\n",
    "#Creating Data Frame as asked for displaying data in readable format\n",
    "\n",
    "df = pd.DataFrame({\"Movie Name \": namelist2,\"Year Of Realese \": realeselist3 , \"Imdb Rating\" : ratinglist5})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e4360e",
   "metadata": {},
   "source": [
    "Write a python program to display list of respected former presidents of India(i.e. Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae537243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>President Name</th>\n",
       "      <th>Term of Office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind</td>\n",
       "      <td>25 July, 2017 to 25 July, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee</td>\n",
       "      <td>25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil</td>\n",
       "      <td>25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam</td>\n",
       "      <td>25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan</td>\n",
       "      <td>25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma</td>\n",
       "      <td>25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman</td>\n",
       "      <td>25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh</td>\n",
       "      <td>25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy</td>\n",
       "      <td>25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed</td>\n",
       "      <td>24 August, 1974 to 11 February, 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri</td>\n",
       "      <td>3 May, 1969 to 20 July, 1969 and 24 August, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain</td>\n",
       "      <td>13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan</td>\n",
       "      <td>13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad</td>\n",
       "      <td>26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   President Name  \\\n",
       "0           Shri Ram Nath Kovind    \n",
       "1          Shri Pranab Mukherjee    \n",
       "2   Smt Pratibha Devisingh Patil    \n",
       "3         DR. A.P.J. Abdul Kalam    \n",
       "4           Shri K. R. Narayanan    \n",
       "5        Dr Shankar Dayal Sharma    \n",
       "6            Shri R Venkataraman    \n",
       "7               Giani Zail Singh    \n",
       "8      Shri Neelam Sanjiva Reddy    \n",
       "9       Dr. Fakhruddin Ali Ahmed    \n",
       "10  Shri Varahagiri Venkata Giri    \n",
       "11              Dr. Zakir Husain    \n",
       "12  Dr. Sarvepalli Radhakrishnan    \n",
       "13           Dr. Rajendra Prasad    \n",
       "\n",
       "                                       Term of Office  \n",
       "0                     25 July, 2017 to 25 July, 2022   \n",
       "1                     25 July, 2012 to 25 July, 2017   \n",
       "2                     25 July, 2007 to 25 July, 2012   \n",
       "3                     25 July, 2002 to 25 July, 2007   \n",
       "4                     25 July, 1997 to 25 July, 2002   \n",
       "5                     25 July, 1992 to 25 July, 1997   \n",
       "6                     25 July, 1987 to 25 July, 1992   \n",
       "7                     25 July, 1982 to 25 July, 1987   \n",
       "8                     25 July, 1977 to 25 July, 1982   \n",
       "9                24 August, 1974 to 11 February, 1977  \n",
       "10  3 May, 1969 to 20 July, 1969 and 24 August, 19...  \n",
       "11                        13 May, 1967 to 3 May, 1969  \n",
       "12                       13 May, 1962 to 13 May, 1967  \n",
       "13                   26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BeautifulSoup Assignment-1 Question-4\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#scraping a site for fetching list of former presidents and Term of Office\n",
    "\n",
    "url_link = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "request = requests.get(url_link)\n",
    "\n",
    "#Creating Empty Lists for working\n",
    "\n",
    "list1=[]\n",
    "namelist=[]\n",
    "termlist=[]\n",
    "\n",
    "soup = BeautifulSoup(request.text, 'html5lib')   #Parsing of Data\n",
    " \n",
    "#Fetching Required data from relevent tags and class\n",
    "\n",
    "for index in soup.find_all('div',class_=\"presidentListing\"):\n",
    "    list1.append(index.text)\n",
    "\n",
    "#Seperating Name and Term , Removing extra data \n",
    "\n",
    "for i in range(0,len(list1)):\n",
    "    name = list1[i].split(\"\\n\")[1]\n",
    "    term=  list1[i].split(\"\\n\")[2]\n",
    "    \n",
    "    namelist.append(name.split(\"(\")[0])       #Removing Birth information\n",
    "    termlist.append(term.split(\": \")[1])      #Removing Extra text before Term of service\n",
    "\n",
    "#printing required data using Data Frame\n",
    "\n",
    "df=pd.DataFrame({\"President Name\": namelist,\"Term of Office\": termlist}) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feadfb1f",
   "metadata": {},
   "source": [
    "Write a python program to scrape cricket rankings from icc-cricket.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2078046",
   "metadata": {},
   "source": [
    "#a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02a7fd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Team Name  Points Matches Ranking\n",
      "0   New Zealand  2,670      23     116\n",
      "1       England  3,400      30     113\n",
      "2     Australia  3,572      32     112\n",
      "3         India  4,098      38     108\n",
      "4      Pakistan  2,354      22     107\n",
      "5  South Africa  2,392      24     100\n",
      "6    Bangladesh  3,129      33      95\n",
      "7     Sri Lanka  2,800      31      90\n",
      "8   Afghanistan  1,419      20      71\n",
      "9   West Indies  2,902      41      71\n"
     ]
    }
   ],
   "source": [
    "#BeautifulSoup Assignment-1 Question-5(a)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# scraping The Icc website for ODI team Ranking(mens)\n",
    "\n",
    "url_link = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "request = requests.get(url_link)\n",
    "\n",
    "#Creating Empty Lists\n",
    "\n",
    "rating_list=[]\n",
    "points_list=[]\n",
    "name_list=[]\n",
    "list4=[]\n",
    "list5=[]\n",
    "\n",
    "soup = BeautifulSoup(request.text, 'html5lib')               #Parsing\n",
    "\n",
    "#Finding Ratings\n",
    "\n",
    "for index in soup.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
    "    rating_list.append(index.text)\n",
    "\n",
    "#Finding Points\n",
    "\n",
    "for index in soup.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
    "    points_list.append(index.text)\n",
    "\n",
    "#Finding Team name\n",
    "\n",
    "for index in soup.find_all('span',class_=\"u-hide-phablet\"):\n",
    "    name_list.append(index.text)   \n",
    "    \n",
    "#Finding Data of First team and appending at first position\n",
    "\n",
    "for index in soup.find_all('td',class_=\"rankings-block__banner--matches\"):\n",
    "    matches1=index.text \n",
    "\n",
    "for index in soup.find_all('td',class_=\"rankings-block__banner--points\"):\n",
    "    points1=index.text\n",
    "\n",
    "for index in soup.find_all('td',class_=\"rankings-block__banner--rating u-text-right\"):\n",
    "    ranking1=index.text.strip()\n",
    "\n",
    "rating_list.insert(0,ranking1)\n",
    "\n",
    "#Seperating Points and matches\n",
    "\n",
    "for i in range(0,len(points_list)):\n",
    "    if(i%2==0):\n",
    "        list4.append(points_list[i])\n",
    "    else:\n",
    "        list5.append(points_list[i])\n",
    "\n",
    "list4.insert(0,matches1)\n",
    "list5.insert(0,points1)\n",
    "\n",
    "#Printing top 10 names using data frames\n",
    "\n",
    "df = pd.DataFrame({\"Team Name \": name_list[0:10],\"Points\": list5[0:10] , \"Matches\" : list4[0:10] ,\"Ranking\":rating_list[0:10]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a618bb84",
   "metadata": {},
   "source": [
    "5(b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "924fe3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Team Name             Player Name Ranking\n",
      "0        PAK             Babar Azam     890\n",
      "1        PAK            Imam-ul-Haq     116\n",
      "2         SA  Rassie van der Dussen     113\n",
      "3         SA        Quinton de Kock     112\n",
      "4        AUS           David Warner     108\n",
      "5        AUS            Steve Smith     107\n",
      "6        ENG         Jonny Bairstow     100\n",
      "7        IND            Virat Kohli      95\n",
      "8        IND           Rohit Sharma      90\n",
      "9         NZ        Kane Williamson      71\n"
     ]
    }
   ],
   "source": [
    "#BeautifulSoup Assignment-1 Question-5(b)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# scraping ICC website for batsman ranking\n",
    "\n",
    "url_link = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "request = requests.get(url_link)\n",
    "\n",
    "#creating Empty Lists\n",
    "\n",
    "player_list=[]\n",
    "points_list=[]\n",
    "name_list=[]\n",
    "list4=[]\n",
    "list5=[]\n",
    "soup = BeautifulSoup(request.text, 'html5lib')\n",
    "\n",
    "#Finding Ratings\n",
    "for index in soup.find_all('td',class_=\"table-body__cell rating\"):\n",
    "    rating_list.append(index.text)\n",
    "\n",
    "#Finding Names\n",
    "\n",
    "for index in soup.find_all('span',class_=\"table-body__logo-text\"):\n",
    "    name_list.append(index.text)\n",
    "\n",
    "#Finding Rankings\n",
    "\n",
    "for index in soup.find_all('td',class_=\"table-body__cell rankings-table__name name\"):\n",
    "    player_list.append(index.text)\n",
    "\n",
    "#Finding Details of top player and appending\n",
    "\n",
    "for index in soup.find_all('div',class_=\"rankings-block__banner--rating\"):\n",
    "    ranking1=index.text.strip()\n",
    "\n",
    "for index in soup.find_all('div',class_=\"rankings-block__banner--name-large\"):\n",
    "    name1=index.text.strip()\n",
    "\n",
    "for index in soup.find_all('div',class_=\"rankings-block__banner--nationality\"):\n",
    "    country1=index.text.strip()\n",
    "for i in range(0,len(player_list)):\n",
    "    name=(player_list[i].strip())\n",
    "    list4.append(name)\n",
    "    \n",
    "name_list.insert(0,country1)\n",
    "list4.insert(0,name1)\n",
    "rating_list.insert(0,ranking1)\n",
    "\n",
    "#Printing top 10 names using data frames\n",
    "\n",
    "df = pd.DataFrame({\"Team Name \": name_list[0:10],\"Player Name\": list4[0:10] , \"Ranking\":rating_list[0:10]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eff4a7",
   "metadata": {},
   "source": [
    "5(c) Top 10 ODI bowlers along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96a46a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Team Name         Player Name Ranking\n",
      "0         NZ        Trent Boult     752\n",
      "1        AUS     Josh Hazlewood     890\n",
      "2        AUS     Mitchell Starc     116\n",
      "3         NZ         Matt Henry     113\n",
      "4        PAK     Shaheen Afridi     112\n",
      "5        AFG        Rashid Khan     108\n",
      "6        AUS         Adam Zampa     107\n",
      "7        BAN    Shakib Al Hasan     100\n",
      "8        BAN  Mustafizur Rahman      95\n",
      "9        AFG   Mujeeb Ur Rahman      90\n"
     ]
    }
   ],
   "source": [
    "#BeautifulSoup Assignment-1 Question-5(c)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# scraping ICC website for bowler ranking\n",
    "\n",
    "url_link = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "request = requests.get(url_link)\n",
    "\n",
    "#creating Empty Lists\n",
    "\n",
    "player_list=[]\n",
    "points_list=[]\n",
    "name_list=[]\n",
    "list4=[]\n",
    "list5=[]\n",
    "soup = BeautifulSoup(request.text, 'html5lib')\n",
    "\n",
    "#Finding Ratings\n",
    "for index in soup.find_all('td',class_=\"table-body__cell rating\"):\n",
    "    rating_list.append(index.text)\n",
    "\n",
    "#Finding Names\n",
    "\n",
    "for index in soup.find_all('span',class_=\"table-body__logo-text\"):\n",
    "    name_list.append(index.text)\n",
    "\n",
    "#Finding Rankings\n",
    "\n",
    "for index in soup.find_all('td',class_=\"table-body__cell rankings-table__name name\"):\n",
    "    player_list.append(index.text)\n",
    "\n",
    "#Finding Details of top player and appending\n",
    "\n",
    "for index in soup.find_all('div',class_=\"rankings-block__banner--rating\"):\n",
    "    ranking1=index.text.strip()\n",
    "\n",
    "for index in soup.find_all('div',class_=\"rankings-block__banner--name-large\"):\n",
    "    name1=index.text.strip()\n",
    "\n",
    "for index in soup.find_all('div',class_=\"rankings-block__banner--nationality\"):\n",
    "    country1=index.text.strip()\n",
    "for i in range(0,len(player_list)):\n",
    "    name=(player_list[i].strip())\n",
    "    list4.append(name)\n",
    "    \n",
    "name_list.insert(0,country1)\n",
    "list4.insert(0,name1)\n",
    "rating_list.insert(0,ranking1)\n",
    "\n",
    "#Printing top 10 names using data frames\n",
    "\n",
    "df = pd.DataFrame({\"Team Name \": name_list[0:10],\"Player Name\": list4[0:10] , \"Ranking\":rating_list[0:10]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ecef3",
   "metadata": {},
   "source": [
    "Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51777182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Team Name  Points Matches Ranking\n",
      "0     Australia  3,061      18     170\n",
      "1       England  3,342      28     119\n",
      "2  South Africa  3,098      26     119\n",
      "3         India  2,820      27     104\n",
      "4   New Zealand  2,553      25     102\n",
      "5   West Indies  2,535      27      94\n",
      "6    Bangladesh    983      13      76\n",
      "7      Thailand    572       8      72\n",
      "8      Pakistan  1,519      24      63\n",
      "9     Sri Lanka    353       8      44\n"
     ]
    }
   ],
   "source": [
    "#BeautifulSoup Assignment-1 Question-6(a)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# scraping The Icc website for ODI team Ranking(women)\n",
    "\n",
    "url_link = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "request = requests.get(url_link)\n",
    "\n",
    "#Creating Empty Lists\n",
    "\n",
    "rating_list=[]\n",
    "points_list=[]\n",
    "name_list=[]\n",
    "list4=[]\n",
    "list5=[]\n",
    "\n",
    "soup = BeautifulSoup(request.text, 'html5lib')               #Parsing\n",
    "\n",
    "#Finding Ratings\n",
    "\n",
    "for index in soup.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
    "    rating_list.append(index.text)\n",
    "\n",
    "#Finding Points\n",
    "\n",
    "for index in soup.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
    "    points_list.append(index.text)\n",
    "\n",
    "#Finding Team name\n",
    "\n",
    "for index in soup.find_all('span',class_=\"u-hide-phablet\"):\n",
    "    name_list.append(index.text)   \n",
    "    \n",
    "#Finding Data of First team and appending at first position\n",
    "\n",
    "for index in soup.find_all('td',class_=\"rankings-block__banner--matches\"):\n",
    "    matches1=index.text \n",
    "\n",
    "for index in soup.find_all('td',class_=\"rankings-block__banner--points\"):\n",
    "    points1=index.text\n",
    "\n",
    "for index in soup.find_all('td',class_=\"rankings-block__banner--rating u-text-right\"):\n",
    "    ranking1=index.text.strip()\n",
    "\n",
    "rating_list.insert(0,ranking1)\n",
    "\n",
    "#Seperating Points and matches\n",
    "\n",
    "for i in range(0,len(points_list)):\n",
    "    if(i%2==0):\n",
    "        list4.append(points_list[i])\n",
    "    else:\n",
    "        list5.append(points_list[i])\n",
    "\n",
    "list4.insert(0,matches1)\n",
    "list5.insert(0,points1)\n",
    "\n",
    "#Printing top 10 names using data frames\n",
    "\n",
    "df = pd.DataFrame({\"Team Name \": name_list[0:10],\"Points\": list5[0:10] , \"Matches\" : list4[0:10] ,\"Ranking\":rating_list[0:10]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070dd363",
   "metadata": {},
   "source": [
    "6(b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc451342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Team Name           Player Name Ranking\n",
      "0        AUS         Alyssa Healy     785\n",
      "1        AUS          Beth Mooney     170\n",
      "2         SA      Laura Wolvaardt     119\n",
      "3        ENG       Natalie Sciver     119\n",
      "4        IND     Harmanpreet Kaur     104\n",
      "5        IND      Smriti Mandhana     102\n",
      "6        AUS          Meg Lanning      94\n",
      "7        AUS       Rachael Haynes      76\n",
      "8         SL  Chamari Athapaththu      72\n",
      "9         NZ    Amy Satterthwaite      63\n"
     ]
    }
   ],
   "source": [
    "#BeautifulSoup Assignment-1 Question-6(b)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# scraping ICC website for batting ranking\n",
    "\n",
    "url_link = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "request = requests.get(url_link)\n",
    "\n",
    "#creating Empty Lists\n",
    "\n",
    "player_list=[]\n",
    "points_list=[]\n",
    "name_list=[]\n",
    "list4=[]\n",
    "list5=[]\n",
    "soup = BeautifulSoup(request.text, 'html5lib')\n",
    "\n",
    "#Finding Ratings\n",
    "for index in soup.find_all('td',class_=\"table-body__cell rating\"):\n",
    "    rating_list.append(index.text)\n",
    "\n",
    "#Finding Names\n",
    "\n",
    "for index in soup.find_all('span',class_=\"table-body__logo-text\"):\n",
    "    name_list.append(index.text)\n",
    "\n",
    "#Finding Rankings\n",
    "\n",
    "for index in soup.find_all('td',class_=\"table-body__cell rankings-table__name name\"):\n",
    "    player_list.append(index.text)\n",
    "\n",
    "#Finding Details of top player and appending\n",
    "\n",
    "for index in soup.find_all('div',class_=\"rankings-block__banner--rating\"):\n",
    "    ranking1=index.text.strip()\n",
    "\n",
    "for index in soup.find_all('div',class_=\"rankings-block__banner--name-large\"):\n",
    "    name1=index.text.strip()\n",
    "\n",
    "for index in soup.find_all('div',class_=\"rankings-block__banner--nationality\"):\n",
    "    country1=index.text.strip()\n",
    "for i in range(0,len(player_list)):\n",
    "    name=(player_list[i].strip())\n",
    "    list4.append(name)\n",
    "    \n",
    "name_list.insert(0,country1)\n",
    "list4.insert(0,name1)\n",
    "rating_list.insert(0,ranking1)\n",
    "\n",
    "#Printing top 10 names using data frames\n",
    "\n",
    "df = pd.DataFrame({\"Team Name \": name_list[0:10],\"Player Name\": list4[0:10] , \"Ranking\":rating_list[0:10]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b162de8c",
   "metadata": {},
   "source": [
    "6(c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d527e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Team Name         Player Name Ranking\n",
      "0        AUS       Ellyse Perry     374\n",
      "1         WI    Hayley Matthews     785\n",
      "2        ENG     Natalie Sciver     170\n",
      "3         SA     Marizanne Kapp     119\n",
      "4         NZ        Amelia Kerr     119\n",
      "5        IND      Deepti Sharma     104\n",
      "6        AUS   Ashleigh Gardner     102\n",
      "7        AUS      Jess Jonassen      94\n",
      "8        IND     Jhulan Goswami      76\n",
      "9        ENG  Sophie Ecclestone      72\n"
     ]
    }
   ],
   "source": [
    "#BeautifulSoup Assignment-1 Question-6(c)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# scraping ICC website for allrounder ranking\n",
    "\n",
    "url_link = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "request = requests.get(url_link)\n",
    "\n",
    "#creating Empty Lists\n",
    "\n",
    "player_list=[]\n",
    "points_list=[]\n",
    "name_list=[]\n",
    "list4=[]\n",
    "list5=[]\n",
    "soup = BeautifulSoup(request.text, 'html5lib')\n",
    "\n",
    "#Finding Ratings\n",
    "for index in soup.find_all('td',class_=\"table-body__cell rating\"):\n",
    "    rating_list.append(index.text)\n",
    "\n",
    "#Finding Names\n",
    "\n",
    "for index in soup.find_all('span',class_=\"table-body__logo-text\"):\n",
    "    name_list.append(index.text)\n",
    "\n",
    "#Finding Rankings\n",
    "\n",
    "for index in soup.find_all('td',class_=\"table-body__cell rankings-table__name name\"):\n",
    "    player_list.append(index.text)\n",
    "\n",
    "#Finding Details of top player and appending\n",
    "\n",
    "for index in soup.find_all('div',class_=\"rankings-block__banner--rating\"):\n",
    "    ranking1=index.text.strip()\n",
    "\n",
    "for index in soup.find_all('div',class_=\"rankings-block__banner--name-large\"):\n",
    "    name1=index.text.strip()\n",
    "\n",
    "for index in soup.find_all('div',class_=\"rankings-block__banner--nationality\"):\n",
    "    country1=index.text.strip()\n",
    "for i in range(0,len(player_list)):\n",
    "    name=(player_list[i].strip())\n",
    "    list4.append(name)\n",
    "    \n",
    "name_list.insert(0,country1)\n",
    "list4.insert(0,name1)\n",
    "rating_list.insert(0,ranking1)\n",
    "\n",
    "#Printing top 10 names using data frames\n",
    "\n",
    "df = pd.DataFrame({\"Team Name \": name_list[0:10],\"Player Name\": list4[0:10] , \"Ranking\":rating_list[0:10]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50666298",
   "metadata": {},
   "source": [
    "Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94ea43e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Time</th>\n",
       "      <th>News Link Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 things to know before the stock market opens...</td>\n",
       "      <td>13 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/5-things-to-kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buy this 'one stop hydrogen shop' with more th...</td>\n",
       "      <td>22 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/buy-this-one-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>London suffers IPO 'drought' as fund raising p...</td>\n",
       "      <td>28 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/london-suffers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barclays upgrades Lennar, says homebuilder can...</td>\n",
       "      <td>50 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/barclays-upgra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decade ahead will be great for investing but w...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/decade-ahead-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Binance CEO says deposits are 'coming back in'...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/binance-ceo-cz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10-year Treasury yield hovers below 3.5% ahead...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/us-treasury-yi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Goldman and Citi love these two tech giants an...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/goldman-and-ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UK inflation falls from 41-year high as fuel p...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/uk-inflation-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Blasts rock Kyiv as drones attack the capital;...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/ukraine-war-li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Morgan Stanley upgrades its 2023 growth outloo...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>European Central Bank set for 50 basis point r...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/european-centr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Britain's new 'winter of discontent' deepens a...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/britains-new-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GoTo shares surged 33% after UBS double upgrad...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/ubs-upgrades-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>European stocks retreat as markets digest infl...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/european-marke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Morgan Stanley weighs in on ChatGPT and the ri...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Jack Dorsey admits mistakes at Twitter, and sa...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/13/jack-dorsey-ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Cramer's lighting round: I like J.M. Smucker o...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/13/cramers-lighti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Dan Niles is positive on this corner of tech, ...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/dan-niles-is-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Mike Wilson explains why his S&amp;P 500 call is m...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Charts suggest it’s time to buy the dips in oi...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/13/charts-suggest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Jim Cramer says crypto and high wages need to ...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/13/cramer-crypto-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Asia-Pacific markets rise on cooler inflation ...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/14/asia-pacific-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Jim Cramer: Constellation Energy and Sempra En...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/13/jim-cramer-say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Stock futures are muted as investors await the...</td>\n",
       "      <td>13 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/13/stock-market-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Musk's Twitter reportedly hasn't paid rent on ...</td>\n",
       "      <td>13 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/13/twitter-report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Pro Picks: Watch all of Tuesday's big stock ca...</td>\n",
       "      <td>13 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/13/pro-picks-watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Google execs warn company reputation could suf...</td>\n",
       "      <td>13 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/13/google-execs-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sam Bankman-Fried denied bail in Bahamas on FT...</td>\n",
       "      <td>14 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/13/sam-bankman-fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The Arctic is getting warmer and stormier, and...</td>\n",
       "      <td>14 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/13/arctic-getting...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline           Time  \\\n",
       "0   5 things to know before the stock market opens...    13 Min Ago   \n",
       "1   Buy this 'one stop hydrogen shop' with more th...    22 Min Ago   \n",
       "2   London suffers IPO 'drought' as fund raising p...    28 Min Ago   \n",
       "3   Barclays upgrades Lennar, says homebuilder can...    50 Min Ago   \n",
       "4   Decade ahead will be great for investing but w...    1 Hour Ago   \n",
       "5   Binance CEO says deposits are 'coming back in'...   2 Hours Ago   \n",
       "6   10-year Treasury yield hovers below 3.5% ahead...   2 Hours Ago   \n",
       "7   Goldman and Citi love these two tech giants an...   4 Hours Ago   \n",
       "8   UK inflation falls from 41-year high as fuel p...   5 Hours Ago   \n",
       "9   Blasts rock Kyiv as drones attack the capital;...   5 Hours Ago   \n",
       "10  Morgan Stanley upgrades its 2023 growth outloo...   5 Hours Ago   \n",
       "11  European Central Bank set for 50 basis point r...   6 Hours Ago   \n",
       "12  Britain's new 'winter of discontent' deepens a...   6 Hours Ago   \n",
       "13  GoTo shares surged 33% after UBS double upgrad...   6 Hours Ago   \n",
       "14  European stocks retreat as markets digest infl...   7 Hours Ago   \n",
       "15  Morgan Stanley weighs in on ChatGPT and the ri...   7 Hours Ago   \n",
       "16  Jack Dorsey admits mistakes at Twitter, and sa...  11 Hours Ago   \n",
       "17  Cramer's lighting round: I like J.M. Smucker o...  11 Hours Ago   \n",
       "18  Dan Niles is positive on this corner of tech, ...  11 Hours Ago   \n",
       "19  Mike Wilson explains why his S&P 500 call is m...  11 Hours Ago   \n",
       "20  Charts suggest it’s time to buy the dips in oi...  12 Hours Ago   \n",
       "21  Jim Cramer says crypto and high wages need to ...  12 Hours Ago   \n",
       "22  Asia-Pacific markets rise on cooler inflation ...  12 Hours Ago   \n",
       "23  Jim Cramer: Constellation Energy and Sempra En...  12 Hours Ago   \n",
       "24  Stock futures are muted as investors await the...  13 Hours Ago   \n",
       "25  Musk's Twitter reportedly hasn't paid rent on ...  13 Hours Ago   \n",
       "26  Pro Picks: Watch all of Tuesday's big stock ca...  13 Hours Ago   \n",
       "27  Google execs warn company reputation could suf...  13 Hours Ago   \n",
       "28  Sam Bankman-Fried denied bail in Bahamas on FT...  14 Hours Ago   \n",
       "29  The Arctic is getting warmer and stormier, and...  14 Hours Ago   \n",
       "\n",
       "                                       News Link Link  \n",
       "0   https://www.cnbc.com/2022/12/14/5-things-to-kn...  \n",
       "1   https://www.cnbc.com/2022/12/14/buy-this-one-s...  \n",
       "2   https://www.cnbc.com/2022/12/14/london-suffers...  \n",
       "3   https://www.cnbc.com/2022/12/14/barclays-upgra...  \n",
       "4   https://www.cnbc.com/2022/12/14/decade-ahead-w...  \n",
       "5   https://www.cnbc.com/2022/12/14/binance-ceo-cz...  \n",
       "6   https://www.cnbc.com/2022/12/14/us-treasury-yi...  \n",
       "7   https://www.cnbc.com/2022/12/14/goldman-and-ci...  \n",
       "8   https://www.cnbc.com/2022/12/14/uk-inflation-f...  \n",
       "9   https://www.cnbc.com/2022/12/14/ukraine-war-li...  \n",
       "10  https://www.cnbc.com/2022/12/14/morgan-stanley...  \n",
       "11  https://www.cnbc.com/2022/12/14/european-centr...  \n",
       "12  https://www.cnbc.com/2022/12/14/britains-new-w...  \n",
       "13  https://www.cnbc.com/2022/12/14/ubs-upgrades-i...  \n",
       "14  https://www.cnbc.com/2022/12/14/european-marke...  \n",
       "15  https://www.cnbc.com/2022/12/14/morgan-stanley...  \n",
       "16  https://www.cnbc.com/2022/12/13/jack-dorsey-ad...  \n",
       "17  https://www.cnbc.com/2022/12/13/cramers-lighti...  \n",
       "18  https://www.cnbc.com/2022/12/14/dan-niles-is-p...  \n",
       "19  https://www.cnbc.com/2022/12/14/morgan-stanley...  \n",
       "20  https://www.cnbc.com/2022/12/13/charts-suggest...  \n",
       "21  https://www.cnbc.com/2022/12/13/cramer-crypto-...  \n",
       "22  https://www.cnbc.com/2022/12/14/asia-pacific-m...  \n",
       "23  https://www.cnbc.com/2022/12/13/jim-cramer-say...  \n",
       "24  https://www.cnbc.com/2022/12/13/stock-market-f...  \n",
       "25  https://www.cnbc.com/2022/12/13/twitter-report...  \n",
       "26  https://www.cnbc.com/2022/12/13/pro-picks-watc...  \n",
       "27  https://www.cnbc.com/2022/12/13/google-execs-w...  \n",
       "28  https://www.cnbc.com/2022/12/13/sam-bankman-fr...  \n",
       "29  https://www.cnbc.com/2022/12/13/arctic-getting...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BeautifulSoup Assignment-1 Question-7\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# scraping a News Portal\n",
    "\n",
    "url_link = 'https://www.cnbc.com/world/?region=world'\n",
    "request = requests.get(url_link)\n",
    "\n",
    "#Creating List for storing Data\n",
    "\n",
    "time_list=[]\n",
    "headline=[]\n",
    "newslink=[]\n",
    "\n",
    "#Parsing\n",
    "\n",
    "soup = BeautifulSoup(request.text, 'html5lib')\n",
    "\n",
    "#Getting Time Details\n",
    "\n",
    "for index in soup.find_all('span',class_=\"LatestNews-wrapper\"):\n",
    "    time_list.append(index.text)\n",
    "\n",
    "#Getting Headlines\n",
    "\n",
    "for index in soup.find_all('a',class_=\"LatestNews-headline\"):\n",
    "    headline.append(index.text)\n",
    "\n",
    "#Getting Link of News\n",
    "\n",
    "for index in soup.find_all('a',class_=\"LatestNews-headline\"):\n",
    "    newslink.append(index.get(\"href\"))\n",
    "\n",
    "#Printing Data using Data Frame\n",
    "\n",
    "df = pd.DataFrame({\"Headline \": headline,\"Time\":time_list, \"News Link Link\" : newslink })\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6ac3c",
   "metadata": {},
   "source": [
    "Write a python program to scrape the details of most downloaded articles from AI in last 90 days. \n",
    "https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details :\n",
    "i) Paper Title \n",
    "ii) Authors\n",
    "iii) Published Date \n",
    "iv) Paper URL \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71bd9a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article Name</th>\n",
       "      <th>Date Uploaded</th>\n",
       "      <th>Authors</th>\n",
       "      <th>URL of Artile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Article Name    Date Uploaded  \\\n",
       "0                                    Reward is enough    October 2021   \n",
       "1                           Making sense of raw input    October 2021   \n",
       "2   Law and logic: A review from an argumentation ...    October 2015   \n",
       "3              Creativity and artificial intelligence     August 1998   \n",
       "4   Artificial cognition for social human–robot in...       June 2017   \n",
       "5   Explanation in artificial intelligence: Insigh...   February 2019   \n",
       "6                       Making sense of sensory input      April 2021   \n",
       "7   Conflict-based search for optimal multi-agent ...   February 2015   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...     August 1999   \n",
       "9   The Hanabi challenge: A new frontier for AI re...      March 2020   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   February 2021   \n",
       "11           Argumentation in artificial intelligence    October 2007   \n",
       "12  Algorithms for computing strategies in two-pla...     August 2016   \n",
       "13      Multiple object tracking: A literature review      April 2021   \n",
       "14  Selection of relevant features and examples in...   December 1997   \n",
       "15  A survey of inverse reinforcement learning: Ch...     August 2021   \n",
       "16  Explaining individual predictions when feature...  September 2021   \n",
       "17  A review of possible effects of cognitive bias...       June 2021   \n",
       "18  Integrating social power into the decision-mak...   December 2016   \n",
       "19  “That's (not) the output I expected!” On the r...  September 2021   \n",
       "20  Explaining black-box classifiers using post-ho...        May 2021   \n",
       "21  Algorithm runtime prediction: Methods & evalua...    January 2014   \n",
       "22              Wrappers for feature subset selection   December 1997   \n",
       "23  Commonsense visual sensemaking for autonomous ...    October 2021   \n",
       "24         Quantum computation, quantum theory and AI   February 2010   \n",
       "\n",
       "                                              Authors  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...   \n",
       "1           Evans, Richard, Bošnjak, Matko and 5 more   \n",
       "2                   Prakken, Henry, Sartor, Giovanni    \n",
       "3                                 Boden, Margaret A.    \n",
       "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more   \n",
       "5                                        Miller, Tim    \n",
       "6   Evans, Richard, Hernández-Orallo, José and 3 more   \n",
       "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   \n",
       "8   Sutton, Richard S., Precup, Doina, Singh, Sati...   \n",
       "9         Bard, Nolan, Foerster, Jakob N. and 13 more   \n",
       "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   \n",
       "11               Bench-Capon, T.J.M., Dunne, Paul E.    \n",
       "12       Bošanský, Branislav, Lisý, Viliam and 3 more   \n",
       "13             Luo, Wenhan, Xing, Junliang and 4 more   \n",
       "14                      Blum, Avrim L., Langley, Pat    \n",
       "15                   Arora, Saurabh, Doshi, Prashant    \n",
       "16      Aas, Kjersti, Jullum, Martin, Løland, Anders    \n",
       "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...   \n",
       "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    \n",
       "19                      Riveiro, Maria, Thill, Serge    \n",
       "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...   \n",
       "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...   \n",
       "22                      Kohavi, Ron, John, George H.    \n",
       "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...   \n",
       "24                                   Ying, Mingsheng    \n",
       "\n",
       "                                        URL of Artile  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BeautifulSoup Assignment-1 Question-8\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "# scraping a site for fetching most downloaded articles\n",
    "\n",
    "url_link = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "request = requests.get(url_link)\n",
    " \n",
    "soup = BeautifulSoup(request.text, 'html5lib')\n",
    "\n",
    "#Creating Empty Lists\n",
    "\n",
    "author_list=[]\n",
    "article_list=[]\n",
    "date_list=[]\n",
    "url_list=[]\n",
    "\n",
    "#Fetching Author\n",
    "\n",
    "for index in soup.find_all('span',class_=\"sc-1w3fpd7-0 dnCnAO\"):\n",
    "    author_list.append(index.text)\n",
    "\n",
    "#Fetching Article\n",
    "\n",
    "for index in soup.find_all('h2',class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\"):\n",
    "    article_list.append(index.text)\n",
    "\n",
    "#Fetching Date of upload\n",
    "\n",
    "for index in soup.find_all('span',class_=\"sc-1thf9ly-2 dvggWt\"):\n",
    "    date_list.append(index.text)\n",
    "\n",
    "#Fetching URL's of Articles\n",
    "\n",
    "for link in soup.find_all('a', class_=\"sc-5smygv-0 fIXTHm\"):\n",
    "    url_list.append(link.get('href'))  \n",
    "\n",
    "df = pd.DataFrame({\"Article Name \": article_list,\"Date Uploaded\": date_list , \"Authors\":author_list ,\"URL of Artile\" : url_list})\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d67b9b",
   "metadata": {},
   "source": [
    "Write a python program to scrape mentioned details from dineout.co.in :\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location \n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d7f6f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Image Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Restaurant Name                         Cuisine  \\\n",
       "0                   Castle Barbeque          Chinese, North Indian   \n",
       "1                   Jungle Jamboree   North Indian, Asian, Italian   \n",
       "2                        Cafe Knosh           Italian, Continental   \n",
       "3                   Castle Barbeque          Chinese, North Indian   \n",
       "4              The Barbeque Company          North Indian, Chinese   \n",
       "5                       India Grill          North Indian, Italian   \n",
       "6                    Delhi Barbeque                   North Indian   \n",
       "7  The Monarch - Bar Be Que Village                   North Indian   \n",
       "8                 Indian Grill Room          North Indian, Mughlai   \n",
       "\n",
       "                                            Location Rating  \\\n",
       "0                     Connaught Place, Central Delhi    4.3   \n",
       "1             3CS Mall,Lajpat Nagar - 3, South Delhi    4.3   \n",
       "2  The Leela Ambience Convention Hotel,Shahdara, ...    4.3   \n",
       "3             Pacific Mall,Tagore Garden, West Delhi    4.3   \n",
       "4                 Gardens Galleria,Sector 38A, Noida    4.3   \n",
       "5               Hilton Garden Inn,Saket, South Delhi    4.3   \n",
       "6     Taurus Sarovar Portico,Mahipalpur, South Delhi    4.3   \n",
       "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad    4.3   \n",
       "8   Suncity Business Tower,Golf Course Road, Gurgaon    4.3   \n",
       "\n",
       "                                          Image Link  \n",
       "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BeautifulSoup Assignment-1 Question-9\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# scraping a restaurent website for various details \n",
    "\n",
    "url_link = 'https://www.dineout.co.in/delhi-restaurants/buffet-special'\n",
    "request = requests.get(url_link)\n",
    "\n",
    "#Creating empty lists\n",
    "\n",
    "name_list=[]\n",
    "rating=[]\n",
    "location=[]\n",
    "cuisine=[]\n",
    "web_list=[]\n",
    "\n",
    "#parsing\n",
    "\n",
    "soup = BeautifulSoup(request.text, 'html5lib')\n",
    "\n",
    "#Getting Restaurent names\n",
    "\n",
    "for index in soup.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "    name_list.append(index.text)\n",
    "\n",
    "#Getting Restaurent location\n",
    "\n",
    "for index in soup.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "    location.append(index.text)\n",
    "\n",
    "#Getting Restaurent cuisines\n",
    "\n",
    "for index in soup.find_all('div',class_=\"detail-info\"):\n",
    "    cuisine.append(index.text.split('|')[1])\n",
    "\n",
    "#rating on site    \n",
    "\n",
    "for index in soup.find_all('div',class_=\"restnt-rating rating-4 hide\"):\n",
    "    rating.append(index.text)   \n",
    "\n",
    "#Getting image URL\n",
    "\n",
    "for index in soup.find_all('img', class_=\"no-img\"):\n",
    "    web_list.append(index.get(\"data-src\"))   \n",
    "\n",
    "\n",
    "#Displaying data using data frames\n",
    "\n",
    "df = pd.DataFrame({\"Restaurant Name \": name_list,\"Cuisine\":cuisine, \"Location\": location ,\"Rating\":rating,\"Image Link\" : web_list })\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd721c",
   "metadata": {},
   "source": [
    "10) Write a python program to scrape the details of top publications from Google Scholar from \n",
    "https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "i) Rank \n",
    "ii) Publication\n",
    "iii) h5-index\n",
    "iv) h5-median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f100f98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ranking</th>\n",
       "      <th>Publication List</th>\n",
       "      <th>H5-Index</th>\n",
       "      <th>H5-Median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Ranking , Publication List, H5-Index, H5-Median]\n",
       "Index: []"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BeautifulSoup Assignment-1 Question-10\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# scraping a website for finding top publications\n",
    "\n",
    "url_link = 'https://scholar.google.com/citations?view_op=top_venues&hl=en'\n",
    "request = requests.get(url_link)\n",
    "ranking_list=[]\n",
    "publication_list=[]\n",
    "h5_list=[]\n",
    "h5_index=[]\n",
    "\n",
    "soup = BeautifulSoup(request.text, 'html5lib')\n",
    "\n",
    "#Finding Ranking\n",
    "\n",
    "for index in soup.find_all('td',class_=\"gsc_mvt_p\"):\n",
    "    ranking_list.append(index.text.replace(\".\",\" \"))\n",
    "\n",
    "#Finding Publication\n",
    "\n",
    "for index in soup.find_all('td',class_=\"gsc_mvt_t\"):\n",
    "    publication_list.append(index.text)\n",
    "\n",
    "#Finding H5 list\n",
    "    \n",
    "for index in soup.find_all('span',class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "    h5_list.append(index.text)\n",
    "\n",
    "#Finding H5 index\n",
    "\n",
    "for index in soup.find_all('a',class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "    h5_index.append(index.text)   \n",
    "\n",
    "#Printing required data\n",
    "\n",
    "df = pd.DataFrame({\"Ranking \": ranking_list,\"Publication List\": publication_list ,\"H5-Index\":h5_index, \"H5-Median\":h5_list})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4efc64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
