{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f986039b",
   "metadata": {},
   "source": [
    "Q.1. Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question-1\n",
    "\n",
    "import selenium\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys                         #importing necessary files\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#initiation of driver\n",
    "driver= webdriver.Chrome(r\"C:\\Users\\Pooja\\Downloads\\chromedriver.exe\")\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "#connecting to the required site\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "#Passing designation as input\n",
    "\n",
    "designation=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[1]/div/div/div/input\")\n",
    "designation.send_keys('Data Analyst')\n",
    "\n",
    "#passing location as input\n",
    "\n",
    "place=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "place.send_keys(\"Bangalore\")\n",
    "\n",
    "#locating search button and clicking\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "time.sleep(5)\n",
    "\n",
    "#Initializing Arrays for storing data\n",
    "\n",
    "job_title= []\n",
    "job_location=[] \n",
    "company_name=[] \n",
    "experience_required=[]\n",
    "\n",
    "title=driver.find_elements(By.XPATH,\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in title[0:10]:\n",
    "    title1=i.text                                                #Creating list of Job-title\n",
    "    job_title.append(title1)\n",
    "\n",
    "\n",
    "location =driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "for i in location[0:10]:\n",
    "    loc=i.text\n",
    "    job_location.append(loc)                                        #Creating list of Job-location\n",
    "\n",
    "\n",
    "company=driver.find_elements(By.XPATH,\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in company[0:10]:\n",
    "    comp=i.text                                    #Creating list of Company name\n",
    "    company_name.append(comp)\n",
    "\n",
    "\n",
    "experience=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi experience']\")\n",
    "for i in experience[0:10]:\n",
    "    exp=i.text                                      ##Creating list of Experience required\n",
    "    experience_required.append(exp)\n",
    "    \n",
    "\n",
    "# Creating Data Frame for presenting data\n",
    "\n",
    "df=pd.DataFrame({\"Job Title \":job_title,\"Job Location \":job_location,\"Company Name \": company_name,\"Experience \":experience_required})\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a72c32a",
   "metadata": {},
   "source": [
    "Q.2. Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b261efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question-2\n",
    "\n",
    "import selenium\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys                         #importing necessary files\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#initiation of driver\n",
    "driver= webdriver.Chrome(r\"C:\\Users\\Pooja\\Downloads\\chromedriver.exe\")\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "#connecting to the required site\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "#Passing designation as input\n",
    "\n",
    "designation=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[1]/div/div/div/input\")\n",
    "designation.send_keys('Data Scientist')\n",
    "\n",
    "#passing location as input\n",
    "\n",
    "place=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "place.send_keys(\"Bangalore\")\n",
    "\n",
    "#locating search button and clicking\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "time.sleep(5)\n",
    "\n",
    "#Initializing Arrays for storing data\n",
    "\n",
    "job_title= []\n",
    "job_location=[] \n",
    "company_name=[] \n",
    "\n",
    "\n",
    "title=driver.find_elements(By.XPATH,\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in title[0:10]:\n",
    "    title1=i.text                                                #Creating list of Job-title\n",
    "    job_title.append(title1)\n",
    "\n",
    "\n",
    "location =driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "for i in location[0:10]:\n",
    "    loc=i.text\n",
    "    job_location.append(loc)                                        #Creating list of Job-location\n",
    "\n",
    "\n",
    "company=driver.find_elements(By.XPATH,\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in company[0:10]:\n",
    "    comp=i.text                                    #Creating list of Company name\n",
    "    company_name.append(comp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creating Data Frame for presenting data\n",
    "\n",
    "df=pd.DataFrame({\"Job Title \":job_title,\"Job Location \":job_location,\"Company Name \": company_name})\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76156df0",
   "metadata": {},
   "source": [
    "Q.3. You have to use the location and salary filter.\n",
    "    You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "    You have to scrape the job-title, job-location, company name, experience required.\n",
    "    The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question-3\n",
    "\n",
    "import selenium\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys                         #importing necessary files\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#initiation of driver\n",
    "driver= webdriver.Chrome(r\"C:\\Users\\Pooja\\Downloads\\chromedriver.exe\")\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "#connecting to the required site\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "#Passing designation as input\n",
    "\n",
    "designation=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[1]/div/div/div/input\")\n",
    "designation.send_keys('Data Scientist')\n",
    "\n",
    "\n",
    "#locating search button and clicking\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "time.sleep(5)\n",
    "\n",
    "#Initializing Arrays for storing data\n",
    "\n",
    "job_title= []\n",
    "job_location=[] \n",
    "company_name=[] \n",
    "\n",
    "#Applling location filter\n",
    "\n",
    "delhi=driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[5]/div[2]/div[2]/label/p/span[1]\")\n",
    "delhi.click()\n",
    "#ActionChains(driver).move_to_element(delhi).click(delhi).perform()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#applying salary filter\n",
    "ctc=driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[6]/div[2]/div[2]/label/p/span[1]\")\n",
    "#ActionChains(driver).move_to_element(ctc).click(ctc).perform()\n",
    "ctc.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "title=driver.find_elements(By.XPATH,\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in title[0:10]:\n",
    "    title1=i.text                                                #Creating list of Job-title\n",
    "    job_title.append(title1)\n",
    "\n",
    "\n",
    "location =driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "for i in location[0:10]:\n",
    "    loc=i.text\n",
    "    job_location.append(loc)                                        #Creating list of Job-location\n",
    "\n",
    "\n",
    "company=driver.find_elements(By.XPATH,\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in company[0:10]:\n",
    "    comp=i.text                                    #Creating list of Company name\n",
    "    company_name.append(comp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creating Data Frame for presenting data\n",
    "\n",
    "df=pd.DataFrame({\"Job Title \":job_title,\"Job Location \":job_location,\"Company Name \": company_name})\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa52a4",
   "metadata": {},
   "source": [
    "Q.4. Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "    1. Brand\n",
    "    2. ProductDescription\n",
    "    3. Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a26d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question-4\n",
    "\n",
    "#importing files\n",
    "\n",
    "import selenium\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#Initiating driver\n",
    "\n",
    "driver= webdriver.Chrome(r\"C:\\Users\\Pooja\\Downloads\\chromedriver.exe\")\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "#Linking Site\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "#Sending text to search bar\n",
    "\n",
    "sunglass=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "sunglass.send_keys('Sunglasses')\n",
    "\n",
    "#Clicking the search button\n",
    "search=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()\n",
    "time.sleep(7)\n",
    "\n",
    "#Declaring required arrays\n",
    "\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "off=[]\n",
    "\n",
    "#Page scroller count\n",
    "\n",
    "for count in range(0,3):\n",
    "    \n",
    "    name=driver.find_elements(By.XPATH,\"//div[@class='_2WkVRV']\")\n",
    "    desc=driver.find_elements(By.XPATH,\"//a[@class='IRpwTa']\")\n",
    "    cost=driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")     #Finding Details of Brand,desciption, price and off\n",
    "    less=driver.find_elements(By.XPATH,\"//div[@class='_3Ay6Sb']\")\n",
    "    \n",
    "    for j in name:\n",
    "        brand.append(j.text)\n",
    "    for j in desc:\n",
    "        description.append(j.text)  \n",
    "    for j in cost:                                     #Creating array of data\n",
    "        price.append(j.text)\n",
    "    for j in less:\n",
    "        off.append(j.text)  \n",
    "  \n",
    " #next page driver  \n",
    "    \n",
    "    next_page=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\")\n",
    "    next_page.click()\n",
    "    time.sleep(5)\n",
    "\n",
    "#Creating DataFrame of required data\n",
    "    \n",
    "    \n",
    "df=pd.DataFrame({\"Brand \":brand[0:100],\"Description\": description[0:100],\"Price\": price[0:100], \"Off \":off[0:100]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34276576",
   "metadata": {},
   "source": [
    "Q.5. Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/product\u0002reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\n",
    "place=FLIPKAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bfa0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question-5\n",
    "\n",
    "#importing necessary files\n",
    "\n",
    "import selenium\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#Driver Initializtion\n",
    "\n",
    "driver= webdriver.Chrome(r\"C:\\Users\\Pooja\\Downloads\\chromedriver.exe\")\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "#connecting Site\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\")\n",
    "\n",
    "#Declaring Required arrays for data storage\n",
    "\n",
    "\n",
    "description=[]\n",
    "rating=[]\n",
    "review=[]\n",
    "\n",
    "#Page Scroller loop\n",
    "for pge in range(0,15):\n",
    "    \n",
    "    desc=driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\")\n",
    "    star=driver.find_elements(By.XPATH,\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    comment=driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']\")       #locating Required data\n",
    "    \n",
    "    for j in desc:\n",
    "        description.append(j.text)\n",
    "    for j in star:\n",
    "        rating.append(j.text)  #Collecting required data in array\n",
    "    for j in comment:\n",
    "        review.append(j.text) \n",
    "    \n",
    "    next_page=driver.find_element(By.XPATH,\"//a[@class='ge-49M _2Kfbh8']\")\n",
    "    next_page.click()                                            #Clicking the next page\n",
    "    time.sleep(3)\n",
    "\n",
    "#Creating DataFrame of required data\n",
    "\n",
    "df=pd.DataFrame({\"Description \":description[0:100],\"Rating\": rating[0:100], \"Full Review \":review[0:100]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479cfb34",
   "metadata": {},
   "source": [
    "Q.6.Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "    You have to scrape 4 attributes of each sneaker:\n",
    "    1. Brand\n",
    "    2. ProductDescription\n",
    "    3. Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb913436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question-6\n",
    "\n",
    "#importing files\n",
    "\n",
    "import selenium\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#Initiating driver\n",
    "\n",
    "driver= webdriver.Chrome(r\"C:\\Users\\Pooja\\Downloads\\chromedriver.exe\")\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "#Linking Site\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "#Sending text to search bar\n",
    "\n",
    "sneakers=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "sneakers.send_keys('Sneakers')\n",
    "\n",
    "#Clicking the search button\n",
    "search=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()\n",
    "time.sleep(7)\n",
    "\n",
    "#Declaring required arrays\n",
    "\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "off=[]\n",
    "\n",
    "#Page scroller count\n",
    "\n",
    "for count in range(0,3):\n",
    "    \n",
    "    name=driver.find_elements(By.XPATH,\"//div[@class='_2WkVRV']\")\n",
    "    desc=driver.find_elements(By.XPATH,\"//a[@class='IRpwTa']\")\n",
    "    cost=driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")     #Finding Details of Brand,desciption, price and off\n",
    "    less=driver.find_elements(By.XPATH,\"//div[@class='_3Ay6Sb']\")\n",
    "    \n",
    "    for j in name:\n",
    "        brand.append(j.text)\n",
    "    for j in desc:\n",
    "        description.append(j.text)  \n",
    "    for j in cost:                                     #Creating array of data\n",
    "        price.append(j.text)\n",
    "    for j in less:\n",
    "        off.append(j.text)  \n",
    "  \n",
    " #next page driver  \n",
    "    \n",
    "    next_page=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\")\n",
    "    next_page.click()\n",
    "    time.sleep(5)\n",
    "\n",
    "#Creating DataFrame of required data\n",
    "    \n",
    "    \n",
    "df=pd.DataFrame({\"Brand \":brand[0:100],\"Description\": description[0:100],\"Price\": price[0:100], \"Off \":off[0:100]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707879b",
   "metadata": {},
   "source": [
    "Q.7.Go to webpage https://www.amazon.in/\n",
    "    Enter “Laptop” in the search field and then click the search icon.Then set CPU Type filter to “Intel Core i7” \n",
    "    After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "    1. Title\n",
    "    2. Ratings\n",
    "    3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fb7bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question-7\n",
    "\n",
    "#Importing required files\n",
    "\n",
    "import selenium\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#initiating driver\n",
    "\n",
    "driver= webdriver.Chrome(r\"C:\\Users\\Pooja\\Downloads\\chromedriver.exe\")\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "#linking Site\n",
    "\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "#passing data in search field\n",
    "\n",
    "laptop=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "laptop.send_keys('Laptop')\n",
    "\n",
    "#Clicking search button\n",
    "\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div[1]/form/div[3]/div/span/input\")\n",
    "search.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#Applying filter of processor, locating the filter and then perform click \n",
    "\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[6]/ul[6]/li[12]/span/a/span\")\n",
    "ActionChains(driver).move_to_element(location).click(location).perform()\n",
    "\n",
    "time.sleep(7)\n",
    "\n",
    "#initializing required arrays\n",
    "\n",
    "laptop_price=[]\n",
    "laptop_name=[]\n",
    "laptop_rating=[]\n",
    "\n",
    "#Fetching name of laptop and craeting array\n",
    "\n",
    "laptop=driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "\n",
    "for i in laptop[:10]:\n",
    "    name=i.text\n",
    "    laptop_name.append(name)\n",
    "\n",
    "#Fetching price of laptop and craeting array\n",
    "\n",
    "price=driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "\n",
    "for i in price[:10]:\n",
    "    cost=i.text\n",
    "    laptop_price.append(cost)\n",
    "\n",
    "#Fetching rating of laptop and craeting array\n",
    "\n",
    "rating=driver.find_elements(By.XPATH,'//span[@class=\"a-size-base\"]')\n",
    "\n",
    "for i in rating[:10]:\n",
    "    star=i.text\n",
    "    laptop_rating.append(star)\n",
    "\n",
    "#Creating data frame of data\n",
    "\n",
    "df=pd.DataFrame({\"Laptop Name \" :laptop_name,\"Price \": laptop_price,\"Ratings\" : laptop_rating})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c44bdf",
   "metadata": {},
   "source": [
    "Q.8. Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "    get the webpage https://www.azquotes.com/ Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a40930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question-8\n",
    "\n",
    "#importing required files\n",
    "\n",
    "import selenium\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#initializing driver\n",
    "\n",
    "driver= webdriver.Chrome(r\"C:\\Users\\Pooja\\Downloads\\chromedriver.exe\")\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "#linking Site\n",
    "\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "\n",
    "#Arrays for storing data\n",
    "\n",
    "author_list=[]\n",
    "quote_list=[]\n",
    "type_list=[]\n",
    "\n",
    "#Clicking on Top Quotes\n",
    "\n",
    "top=driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a\")\n",
    "top.click()\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "#Page Turner count\n",
    "\n",
    "for count in range(1,11):\n",
    "    \n",
    "    author=driver.find_elements(By.XPATH,\"//div[@class='author']\")\n",
    "    for i in author:\n",
    "        auth=i.text                                  #Finding and creating list of authors\n",
    "        author_list.append(auth)\n",
    "\n",
    "    quote=driver.find_elements(By.XPATH,\"//a[@class='title']\")\n",
    "    for i in quote:\n",
    "        quot=i.text                                  #Finding and creating list of quotations\n",
    "        quote_list.append(quot)\n",
    "\n",
    "    qtype=driver.find_elements(By.XPATH,\"//div[@class='tags']\")\n",
    "    for i in qtype:\n",
    "        types=i.text                                   ##Finding and creating list of type of quotations\n",
    "        type_list.append(types)\n",
    "\n",
    "    \n",
    "    next_page=driver.find_element(By.CLASS_NAME,\"next\" )\n",
    "    \n",
    "    if 'inactive' in next_page.get_attribute('class'):          #Clicking next page untill the next button is active\n",
    "        break;\n",
    "    next_page.click()\n",
    "    \n",
    "    time.sleep(3)\n",
    "\n",
    "#Creating DataFrame for data\n",
    "\n",
    "df=pd.DataFrame({\"Author Name \" :author_list,\"Quotation \": quote_list,\"Type of Quote\" : type_list})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a87a2",
   "metadata": {},
   "source": [
    "Q.9. Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead,\n",
    "Term of office, Remarks) from https://www.jagranjosh.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37def55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question-9\n",
    "\n",
    "#importing required files\n",
    "\n",
    "import selenium\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#Initiating driver\n",
    "\n",
    "driver= webdriver.Chrome(r\"C:\\Users\\Pooja\\Downloads\\chromedriver.exe\")\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "#Connecting Site\n",
    "\n",
    "driver.get(\"https://www.jagranjosh.com/\")\n",
    "\n",
    "#Declaring Array\n",
    "\n",
    "desc=[]\n",
    "\n",
    "#Clicking the GK option\n",
    "\n",
    "gk=driver.find_element(By.XPATH,\"/html/body/div/div[1]/div/div[1]/div/div[6]/div/div[1]/header/div[3]/ul/li[9]/a\")\n",
    "gk.click()\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "#Clicking the List of all Prime Ministers of India\n",
    "\n",
    "pm_list=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div/div[2]/div/div[10]/div/div/ul/li[2]/a\")\n",
    "pm_list.click()\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "#Fetching the data of Prime Ministers\n",
    "\n",
    "temp=driver.find_elements(By.XPATH,\"//*[@id='itemdiv']/div[4]/span/div[2]/table/tbody/tr\")\n",
    "\n",
    "for i in temp:\n",
    "    desc1=i.text\n",
    "    desc.append(desc1)\n",
    "\n",
    "#Creating arrays and bifurcating data as required\n",
    "\n",
    "name_list=[]\n",
    "age_list=[]\n",
    "comment_list=[]\n",
    "term_list=[]\n",
    "\n",
    "n=len(desc) \n",
    "\n",
    "for i in range(1,n):\n",
    "\n",
    "    details=desc[i].split(\"\\n\")\n",
    "    name=details[1]               #Splitting data to bifurcate Name, birth-death, term of office and comment from whole list\n",
    "    name_list.append(name)\n",
    "    \n",
    "    birth=details[2]\n",
    "    age=birth.replace(\"(\",\"\")    #Replacing bracets of birth-death\n",
    "    age_list.append(age.replace(\")\",\"\"))\n",
    "    \n",
    "    comment=details[5:]\n",
    "    comment_list.append(comment)\n",
    "    \n",
    "    term=details[3:5]\n",
    "    term_list.append(term)\n",
    "\n",
    "#formatting data of last PM , seperating age and description\n",
    "\n",
    "m=len(term_list)\n",
    "final=term_list[m-1]\n",
    "term_list[m-1]=final[0]\n",
    "comment_list[m-1]=final[1]\n",
    "\n",
    "#Creating DataFrame\n",
    "\n",
    "df=pd.DataFrame({\"Name \" :name_list,\"Born-Dead \": age_list,\"Term of Office\" : term_list,\"Remarks\" : comment_list})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833596b5",
   "metadata": {},
   "source": [
    "Q.10. Write a python program to display list of 50 Most expensive cars in the world (i.e.\n",
    "Car name ,Description and Price) from https://www.motor1.com/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question-10\n",
    "\n",
    "#Importing required files\n",
    "\n",
    "import selenium\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "#Driver Initialization\n",
    "\n",
    "driver= webdriver.Chrome(r\"C:\\Users\\Pooja\\Downloads\\chromedriver.exe\")\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "#Linking Site\n",
    "\n",
    "driver.get(\"https://www.motor1.com/\")\n",
    "\n",
    "#Clicking on the List option from Dropdown menu on leftside.\n",
    "\n",
    "index=driver.find_element(By.XPATH,\"//div[@class='m1-hamburger-button']\")\n",
    "index.click()\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "#Clicking Feauters drop down button \n",
    "\n",
    "newindex=driver.find_element(By.XPATH,\"/html/body/div[4]/div[1]/div[3]/ul/li[5]/button\")\n",
    "newindex.click()\n",
    "\n",
    "#click on list to find the article\n",
    "\n",
    "index2=driver.find_element(By.XPATH,\"/html/body/div[4]/div[1]/div[3]/ul/li[6]/ul/li[1]/a\")\n",
    "index2.click()\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "#click on 50 most expensive cars in the world article\n",
    "\n",
    "index3=driver.find_element(By.XPATH,\"/html/body/div[3]/div[9]/div[1]/div[1]/div/div/div[2]/div/div[1]/h3/a\")\n",
    "index3.click()\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "car_name=[]\n",
    "temp1=[]\n",
    "price=[]\n",
    "comment=[]\n",
    "fprice=[]\n",
    "\n",
    "#Finding Names of car\n",
    "\n",
    "names=driver.find_elements(By.XPATH,\"//h3[@class='subheader']\")\n",
    "\n",
    "for i in names:\n",
    "    car=i.text\n",
    "    car_name.append(car)\n",
    "\n",
    "#Finding price and description\n",
    "\n",
    "temp=driver.find_elements(By.XPATH,\"//*[@id='article_box']/div[1]/div[2]/div[1]/p\")\n",
    "\n",
    "for i in temp:\n",
    "    desc=i.text              #Creating Array of data\n",
    "    temp1.append(desc)\n",
    "\n",
    "total=len(temp1)\n",
    "\n",
    "for i in range(3,total):\n",
    "    if(i%2==0):                          #Bifurcating Comments\n",
    "        comment.append(temp[i].text)\n",
    "    else:\n",
    "        price.append(temp[i].text)        #Bifurcating Price\n",
    "\n",
    "t=len(price)\n",
    "\n",
    "for i in range(0,t):\n",
    "    nprice=price[i].split(\"$\")   #Splitting unrequired data\n",
    "    fprice.append(nprice[1])\n",
    "\n",
    "n=len(car_name)-1   \n",
    "\n",
    "#Creating DataFrame\n",
    "\n",
    "df=pd.DataFrame({\"Name \": car_name[0:n], \"Price in $ \": fprice, \"Description \":comment})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b236596a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
